\documentclass[a4paper]{article}
\usepackage[spanish,es-tabla]{babel}	% trabajar en español
\spanishsignitems	
%\usepackage{simplemargins}

%\usepackage[square]{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{hyperref}
\usepackage{amsthm}
\newtheorem{theorem}{Teorema}
\newtheorem{lemma}{Lema}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
%\usepackage{algorithm2e}



\begin{document}
\pagenumbering{arabic}

\Large
 \begin{center}
Método de Diferencias Finitas\\


\hspace{10pt}

% Author names and affiliations
\large
%Lic. Julio A. Medina$^1$ \\
Julio A. Medina\\
\hspace{10pt}
\small  
Universidad de San Carlos\\
Escuela de Ciencias Físicas y Matemáticas\\
Maestría en Física\\
\href{mailto:julioantonio.medina@gmail.com}{julioantonio.medina@gmail.com}\\

\end{center}

\hspace{10pt}

\normalsize
\section{Ecuaciones diferenciales parciales elípticas}
La ecuación diferencial parcial elíptica a considerar es la ecuación de Poisson
\begin{equation}\label{eq::Poisson}
\nabla^2 u(x,y)\equiv \frac{\partial^2 u}{\partial x^2}(x,y) + \frac{\partial^2 u}{\partial y^2}(x,y)=f(x,y)
\end{equation}
en $R=\{ (x,y)\,\,|\,\, a<x<b ,\, c<y<d \}$, con $u(x,y)=g(x,y) \in S$, donde $S$ denota al contorno de $R$. Si $f$ y $g$ son continuas en su dominio entonces hay una única solución a la ecuación.
\subsection{Seleccionando un retículo}
El método a utilizar es una adaptación bidimensional del método de diferencias finitas para problemas con fronteras lineales como se discute en \cite{Burden}. El primer paso es escoger enteros $n$ y $m$ para definir el tamaño de los pasos(\textit{steps}) $h=(b-a)/n$ y $k=(d-c)/m$ particionando de está manera el intervalo $[a,b]$ en $n$ partes iguales de ancho $h$ y el intervalo $[c,d]$ en $m$ partes iguales con ancho $k$, formando un retículo o cuadricula como se puede ver en la figura

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.29]{./lattice.png} 
\end{center} 
\caption{Cuadrícula de $n\times m$}
\label{fig::fig1}
\end{figure}
Este retículo se construye formalmente al dibujar lineas verticales y horizontales sobre el dentro del rectángulo $R$ en los puntos con coordenadas $(x_i, y_j)$, donde
\begin{equation}
x_i=a+ih,\,\,\,\text{para cada }i=0,1,2,\hdots,n\, \text{ y }\, y=a+jk,\,\,\,\text{para cada }j=0,1,2,\hdots,m
\end{equation}
Las rectas correspondientes a $x=x_i$ y $y=y_i$ son las lineas que forman la cuadricula y sus intersecciones son los puntos del retículo. Para cada punto interior del retículo se $(x_i,y_j)$, para $i=1,2,\hdots,n-1$ y $j=1,2,\hdots,m-1$, se puede utilizar una serie de Taylor en la variable $x$ alrededor del punto $x_i$ para generar una fórmula de diferencia centrada
\begin{equation}
\frac{\partial^2 u}{\partial x^2}(x_i,y_i)=\frac{u(x_{i+1},y_j)-2u(x_i,y_i)+u(x_{i-1},y_j)}{h^2}-\frac{h^2}{12}\frac{\partial^4 u}{\partial x^4}(\xi_i,y_j)
\end{equation}
donde $\xi_i \in (x_{i-1},x_{i+1})$. De igual manera se puede encontrar la serie de Taylor en la variable $y$ alrededor del punto $y_j$ para hallar la diferencia centrada
\begin{equation}
\frac{\partial^2 u}{\partial y^2}(x_i,y_i)=\frac{u(x_{i},y_{j+1})-2u(x_i,y_i)+u(x_{i},y_{j-1})}{k^2}-\frac{k^2}{12}\frac{\partial^4 u}{\partial y^4}(x_i,\eta_j)
\end{equation}
donde $\eta_i \in (y_{i-1},y_{i+1})$, sustituyendo estas fórmulas en las ecuación \ref{eq::Poisson} permite expresar la ecuación de Poisson en los puntos $(x_i,y_j)$ como
\begin{equation}
\begin{aligned}
&\frac{u(x_{i+1},y_j)-2u(x_i,y_i)+u(x_{i-1},y_j)}{h^2}+\frac{u(x_{i},y_{j+1})-2u(x_i,y_i)+u(x_{i},y_{j-1})}{k^2}\\
&=f(x_i,y_j)+\frac{h^2}{12}\frac{\partial^4 u}{\partial x^4}(\xi_i,y_j)+\frac{k^2}{12}\frac{\partial^4 u}{\partial y^4}(x_i,\eta_j)
\end{aligned}
\end{equation}
para cada $i=1,2,\hdots,n-1$ y $j=1,2,\hdots,m-1$. Las condiciones de contorno son
\begin{equation*}
u(x_0,y_j)=g(x_0,y_j)\,\, \text{ y } \,\, u(x_n,y_j)=g(x_n,y_j),\,\,\, \text{para cada }j=0,1,\hdots,m;
\end{equation*}
\begin{equation*}
u(x_i,y_0)=g(x_i,y_0)\,\, \text{ y } \,\, u(x_i,y_m)=g(x_i,y_m),\,\,\, \text{para cada }i=1,2,\hdots,n-1;
\end{equation*}
\subsection{Método de Diferencias Finitas}
En forma de ecuación de diferencias el Método de diferencias finitas es
\begin{equation}
2\Big[\big(\frac{h}{k}\big)^2 +1  \Big]w_{ij}-(w_{i+1,j}+w_{i-1,j})-\big(\frac{h}{k}\big)^2(w_{i,j+1}+w_{i,j-1})=-h^2 f(x_i,y_j)
\end{equation}
para cada $i=1,2,\hdots,n-1$ y $j=1,2,\hdots,m-1$, donde
\begin{equation}
\begin{aligned}
&w_{0j}=g(x_0,y_j)\,\,\text{y }\, w_{nj}=g(x_n,y_j),\,\,\, \text{para cada } j=0,1,\hdots,m;\\
&w_{i0}=g(x_i,y_0)\,\,\text{y }\, w_{im}=g(x_i,y_m),\,\,\, \text{para cada } i=1,2,\hdots,n-1
\end{aligned}
\end{equation}
 



\section{Método de Diferencias Finitas para ecuación de Poisson}

\subsection{Algoritmo diferencias finitas(sistema lineal)}
\begin{algorithm}[H]
\caption{Función L para re-etiquetar las variables del sistema lineal}
\begin{algorithmic}[H]
\Function L {$i, j, n, m$}
\State \Return $(i+1) + (m-1-(j+1)) \times (n-1) - 1$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Finite Difference Linear System}
\begin{algorithmic}[H]
%\Function{finite\_difference\_linear\_system}{$a,b,c,d,n,m,f,g$}
\State $h \gets (b-a)/n$
\State $k \gets (d-c)/n$
\State $x \gets \text{numpy.linspace}(a, b, n+1)$
\State $y \gets \text{numpy.linspace}(c, d, m+1)$
\State $W_{ij} \gets$ zeros $(((n-1)\times(m-1)),((n-1)\times(m-1)))$
\State $w \gets$ zeros $((n-1)\times(m-1))$
\For{$i \gets 0$ to $n-2$}
\For{$j \gets 0$ to $m-2$}
\State $W_{ij}[l(i,j,n,m), l(i,j,n,m)] \gets 2((h/k)^2 + 1)$
\If{$i \neq 0$ and $i \neq n-2$}
\State $W_{ij}[L(i,j,n,m), L(i+1,j,n,m)] \gets -1$
\State $W_{ij}[L(i,j,n,m), L(i-1,j,n,m)] \gets -1$
\Else
\If{$i = 0$}
\State $W_{ij}[L(i,j,n,m), L(i+1,j,n,m)] \gets -1$
\State $w[l(i,j,n,m)] \gets g(x[i], y[j+1], a, b, c, d)$
\EndIf
\If{$i = n-2$}
\State $W_{ij}[L(i,j,n,m), L(i-1,j,n,m)] \gets -1$
\State $w[L(i,j,n,m)] \gets g(x[i+2], y[j+1], a, b, c, d)$
\EndIf
\EndIf
\If{$j \neq 0$ and $j \neq m-2$}
\State $W_{ij}[L(i,j,n,m), L(i,j+1,n,m)] \gets -1$
\State $W_{ij}[L(i,j,n,m), L(i,j-1,n,m)] \gets -1$
\Else
\If{$j = 0$}
\State $W_{ij}[L(i,j,n,m), L(i,j+1,n,m)] \gets -(h/k)^2$
\State $w[L(i,j,n,m)] \gets g(x[i+1], y[j], a, b, c, d)$
\EndIf
\If{$j = m-2$}
\State $W_{ij}[L(i,j,n,m), L(i,j-1,n,m)] \gets -(h/k)^2$
\State $w[L(i,j,n,m)] \gets g(x[i+1], y[j+2], a, b, c, d)$
\EndIf
\EndIf
\State $w[L(i,j,n,m)] \gets -h^2f(x[i], y[j])$
\EndFor
\EndFor
\State \textbf{return} $W_{ij}, w$
\end{algorithmic}
\end{algorithm}

\subsection{Algoritmo de Generalización Factorización de Crout para matrices tridiagonales por bloques}

\begin{algorithm}[H]
\caption{Crout Generalization Algorithm for Tridiagonal Block Matrices}
\begin{algorithmic}[H]
\Require $A \in \mathbb{R}^{N \times N}$, $K \in \mathbb{R}^{N}$, $n \in \mathbb{N}$
\Ensure $sol \in \mathbb{R}^{N}$
\State $N \gets \lfloor \frac{\operatorname{shape}(A)[1]}{n} \rfloor$
\State $W \gets []$
\State $B_1 \gets A_{1:n,1:n}$
\State $C_1 \gets A_{1:n,n:n+n}$
\State $W.append(\operatorname{inv}(B_1) \cdot C_1)$
\For{$i \gets 2$ to $N-1$}
\State $B_i \gets A_{(i-1)n:in,(i-1)n:in}$
\State $A_i \gets A_{(i-1)n:in,(i-2)n:(i-1)n}$
\State $C_i \gets A_{(i-1)n:in,in+1:n+(i-1)n}$
\State $W.append(\operatorname{inv}(B_i - A_i \cdot W_{i-2}) \cdot C_i)$
\EndFor
\State $B_N \gets A_{1:n,1:n}$
\State $K_1 \gets K_{1:n}$
\State $G_1 \gets \operatorname{inv}(B_1) \cdot K_1$
\State $G \gets []$
\State $G.append(G_1)$
\For{$i \gets 2$ to $N$}
\State $K_i \gets K_{(i-1)n:in}$
\State $B_i \gets A_{(i-1)n:in,(i-1)n:in}$
\State $A_i \gets A_{(i-1)n:in,(i-2)n:(i-1)n}$
\State $G.append(\operatorname{inv}(B_i - A_i \cdot W_{i-2}) \cdot (K_i - A_i \cdot G_{i-2}))$
\EndFor
\State $Z \gets G[:]$
\For{$i \gets N-2$ to $0$ step $-1$}
\State $Z_i \gets G_i - W_i \cdot Z_{i+1}$
\EndFor
\State $sol \gets \operatorname{concatenate}(Z)$
\State \textbf{return} $sol$
\end{algorithmic}
\end{algorithm}
\subsection{Método iterativo SOR para resolver sistemas lineales}
\begin{algorithm}[H]
%\caption{Método iterativo SOR para resolver sistemas lineales}
\caption{Iterative SOR method for solving linear systems}
\begin{algorithmic}[H]
\Require{$A$ es una $n \times n$ matriz, $b$ es un vector de incógnitas de dimensión $n$, $x^{(0)}$ es una solución propuesta inicial, $\omega$ es el parámetro de relajamiento, $\epsilon$ es la tolerancia, and $max_{iter}$ es el número máximo de iteraciones.}
\Ensure{$x$ es una solución aproximada de $Ax = b$.}
\State $k \gets 1$
\State $xo \gets x^{(0)}$
\State $tolerance\gets \textbf{True}$
\While{$k < max_{iter}$ and $\text{tolerance}$}
\For{$i = 1$ to $n$}
\State $s \gets \sum_{j=1}^{i-1} a_{ij}x_j + \sum_{j=i+1}^{n} a_{ij}xo_j$
\State $x_i \gets (1-\omega) xo_i + \frac{\omega}{a_{ii}} (b_i - s)$
\EndFor
\If{$||x - xo|| < \epsilon$}
\State $tolerance\gets \textbf{False}$
\EndIf
\State $k \gets k+1$
\State $xo=x$
\EndWhile
\If{$k = max_{iter}$}
\State \textbf{print} "Se ha alcanzado el número máximo de iteraciones"
\EndIf
\State \textbf{return} $x$
\end{algorithmic}
\end{algorithm}
%En este trabajo de graduación consta de dos partes, en la primera parte se ha realizado una investigación sobre el enfoque de la mecánica estadística en la redes neuronales. En la segunda parte
\begin{thebibliography}{99}
%% La bibliografía se ordena en orden alfabético respecto al apellido del 
%% autor o autor principal
%% cada entrada tiene su formatado dependiendo si es libro, artículo,
%% tesis, contenido en la web, etc


\bibitem{Burden} Richard L. Burden, J. Douglas Faires \textit{Numerical Analysis}, (Ninth Edition). Brooks/Cole, Cengage Learning. 978-0-538-73351-9

%\bibitem{Feynman} 
%\bibitem{Hopfield} J.J. Hopfield. \textit{Neural Networks and physical systems with emergent collective computational abilities}. \url{https://doi.org/10.1073/pnas.79.8.2554}


%\bibitem{McCulloch} Warren S. McChulloch, Walter H. Pitts. \textit{A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY}. \url{http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf}



\end{thebibliography}
\end{document}

